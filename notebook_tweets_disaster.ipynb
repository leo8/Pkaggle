{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b869fa",
   "metadata": {},
   "source": [
    "# Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294d328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da381c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings\n",
    "\n",
    "from sentence_transformers import util, SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811665fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep learning\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324fd909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce0456e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model selection\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fde2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT for Sequence Classification\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cac0f3",
   "metadata": {},
   "source": [
    "# Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02d0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329023fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b86943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9f14827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb0401",
   "metadata": {},
   "source": [
    "### Split training data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a51c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e6db19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3339e823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab1839",
   "metadata": {},
   "source": [
    "### Resetting index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "108c4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f432925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4266</td>\n",
       "      <td>drowning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@HeyImBeeYT its like theres fire in my skin an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10445</td>\n",
       "      <td>wild%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some great footage of STRONG work from San Ber...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6296</td>\n",
       "      <td>hostage</td>\n",
       "      <td>Starling City</td>\n",
       "      <td>That moth that held me hostage yesterday has b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2553</td>\n",
       "      <td>crash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Next Financial Crash. ÛÏThe Writing is on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All of this energy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       keyword       location  \\\n",
       "0   4266      drowning            NaN   \n",
       "1  10445  wild%20fires            NaN   \n",
       "2   6296       hostage  Starling City   \n",
       "3   2553         crash            NaN   \n",
       "4  10005       tsunami            NaN   \n",
       "\n",
       "                                                text  target  \n",
       "0  @HeyImBeeYT its like theres fire in my skin an...       1  \n",
       "1  Some great footage of STRONG work from San Ber...       1  \n",
       "2  That moth that held me hostage yesterday has b...       0  \n",
       "3  The Next Financial Crash. ÛÏThe Writing is on...       0  \n",
       "4                                 All of this energy       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b04e231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f527c53",
   "metadata": {},
   "source": [
    "### Separating X and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2facf0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1191684",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d794d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0139e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = val_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d9a52",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86d9eefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a1775b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keyword.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f450710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2533"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.location.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1e9f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_full_text = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    full_text = data['text'][i]\n",
    "    if str(data['keyword'][i]) != 'nan':\n",
    "        full_text += f' #{data[\"keyword\"][i]}'\n",
    "    if str(data['location'][i]) != 'nan':\n",
    "        full_text += f' #{data[\"location\"][i]}'\n",
    "\n",
    "    l_full_text.append(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78efc842",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['full_text'] = pd.Series(l_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df2344d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                          full_text  \n",
       "0       1  Our Deeds are the Reason of this #earthquake M...  \n",
       "1       1             Forest fire near La Ronge Sask. Canada  \n",
       "2       1  All residents asked to 'shelter in place' are ...  \n",
       "3       1  13,000 people receive #wildfires evacuation or...  \n",
       "4       1  Just got sent this photo from Ruby #Alaska as ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88172477",
   "metadata": {},
   "source": [
    "Après quelques essais, les embeddings de full_text offrent de moins bons résultats de prédiction que ceux du texte simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510950d8",
   "metadata": {},
   "source": [
    "# Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eada19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3025db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = transformer.encode(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6e42f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 768)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0ae147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings = transformer.encode(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e12a4",
   "metadata": {},
   "source": [
    "# Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1398d",
   "metadata": {},
   "source": [
    "### Convert embeddings and target to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a22c4583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-03 10:55:26.484353: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-09-03 10:55:26.489045: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "tensor_embeddings = tf.convert_to_tensor(embeddings)\n",
    "tensor_y = tf.convert_to_tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e7b17f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6851, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0444a96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6851])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ede12669",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_val_embeddings = tf.convert_to_tensor(val_embeddings)\n",
    "tensor_val_y = tf.convert_to_tensor(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd82a7",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfe1e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff15ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.RMSprop(learning_rate=0.001, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "397f8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Define model architecture.\n",
    "model.add(Dense(728, input_shape=(768,), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e09ac0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 728)               559832    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 728)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               186624    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 762,969\n",
      "Trainable params: 762,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c4ea018",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer, #or optimizer\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431dccc",
   "metadata": {},
   "source": [
    "### Training and performances visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39b13247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-03 10:55:28.793517: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-09-03 10:55:30.582938: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - ETA: 0s - loss: 0.4415 - accuracy: 0.8104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-03 10:55:42.588349: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 15s 34ms/step - loss: 0.4415 - accuracy: 0.8104 - val_loss: 0.4251 - val_accuracy: 0.8228 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 14s 63ms/step - loss: 0.3827 - accuracy: 0.8394 - val_loss: 0.4565 - val_accuracy: 0.8228 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.3408 - accuracy: 0.8543\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "215/215 [==============================] - 10s 46ms/step - loss: 0.3407 - accuracy: 0.8545 - val_loss: 0.4559 - val_accuracy: 0.8136 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.2748 - accuracy: 0.8861 - val_loss: 0.4923 - val_accuracy: 0.8176 - lr: 5.0000e-04\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 12s 56ms/step - loss: 0.2250 - accuracy: 0.9047 - val_loss: 0.5203 - val_accuracy: 0.8241 - lr: 5.0000e-04\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 8s 35ms/step - loss: 0.1816 - accuracy: 0.9241 - val_loss: 0.6043 - val_accuracy: 0.8071 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9410\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "215/215 [==============================] - 12s 57ms/step - loss: 0.1472 - accuracy: 0.9410 - val_loss: 0.7155 - val_accuracy: 0.8071 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.1046 - accuracy: 0.9602 - val_loss: 0.7526 - val_accuracy: 0.8176 - lr: 2.5000e-04\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9685\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "215/215 [==============================] - 7s 32ms/step - loss: 0.0849 - accuracy: 0.9685 - val_loss: 0.8387 - val_accuracy: 0.8110 - lr: 2.5000e-04\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 0.0643 - accuracy: 0.9758 - val_loss: 0.8526 - val_accuracy: 0.8084 - lr: 1.2500e-04\n",
      "Epoch 11/20\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.0600 - accuracy: 0.9777\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "215/215 [==============================] - 9s 41ms/step - loss: 0.0600 - accuracy: 0.9777 - val_loss: 0.8968 - val_accuracy: 0.8084 - lr: 1.2500e-04\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 11s 51ms/step - loss: 0.0502 - accuracy: 0.9819 - val_loss: 0.9296 - val_accuracy: 0.8045 - lr: 6.2500e-05\n",
      "Epoch 13/20\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.0493 - accuracy: 0.9806\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "215/215 [==============================] - 10s 46ms/step - loss: 0.0493 - accuracy: 0.9806 - val_loss: 0.9492 - val_accuracy: 0.8084 - lr: 6.2500e-05\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 7s 33ms/step - loss: 0.0434 - accuracy: 0.9828 - val_loss: 0.9724 - val_accuracy: 0.8097 - lr: 3.1250e-05\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9838\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "215/215 [==============================] - 15s 72ms/step - loss: 0.0431 - accuracy: 0.9838 - val_loss: 0.9845 - val_accuracy: 0.8097 - lr: 3.1250e-05\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 7s 31ms/step - loss: 0.0423 - accuracy: 0.9847 - val_loss: 0.9941 - val_accuracy: 0.8084 - lr: 1.5625e-05\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9841\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "215/215 [==============================] - 13s 60ms/step - loss: 0.0408 - accuracy: 0.9841 - val_loss: 0.9953 - val_accuracy: 0.8084 - lr: 1.5625e-05\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 11s 53ms/step - loss: 0.0414 - accuracy: 0.9841 - val_loss: 0.9980 - val_accuracy: 0.8071 - lr: 1.0000e-05\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 6s 30ms/step - loss: 0.0390 - accuracy: 0.9857 - val_loss: 1.0042 - val_accuracy: 0.8097 - lr: 1.0000e-05\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 12s 57ms/step - loss: 0.0416 - accuracy: 0.9837 - val_loss: 1.0050 - val_accuracy: 0.8097 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=tensor_embeddings,\n",
    "    y=tensor_y,\n",
    "    validation_data=(tensor_val_embeddings, tensor_val_y),\n",
    "    callbacks=[learning_rate_reduction],\n",
    "    epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b54d8b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(history, model_name):\n",
    "    \n",
    "    fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    \n",
    "    epochs_nb = history.params['epochs']\n",
    "    \n",
    "    ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    ax_loss.set_title(f'Loss history of {model_name} over {epochs_nb} epochs')\n",
    "    ax_loss.set_xlabel('Number of epochs')\n",
    "    ax_loss.set_ylabel('Loss')\n",
    "    ax_loss.legend()\n",
    "    \n",
    "    ax_acc.plot(history.epoch, history.history[\"accuracy\"], label=\"Train accuracy\")\n",
    "    ax_acc.plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation accuracy\")\n",
    "    ax_acc.set_title(f'Accuracy history of {model_name} over {epochs_nb} epochs')\n",
    "    ax_acc.set_xlabel('Number of epochs')\n",
    "    ax_acc.set_ylabel('Accuracy')\n",
    "    ax_acc.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb76eb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_results(history, 'basic_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfe83a",
   "metadata": {},
   "source": [
    "# Classifier approaches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0945c5",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc32d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b2b71c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   37.2s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3702802c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.997372646329003"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.score(embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ac9c2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8031496062992126"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.score(val_embeddings, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2887e7",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8dce36cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0af757f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3216            4.40m\n",
      "         2           1.2826            4.39m\n",
      "         3           1.2478            4.69m\n",
      "         4           1.2159            5.01m\n",
      "         5           1.1873            5.15m\n",
      "         6           1.1605            5.20m\n",
      "         7           1.1359            5.12m\n",
      "         8           1.1140            5.13m\n",
      "         9           1.0924            5.04m\n",
      "        10           1.0732            5.04m\n",
      "        20           0.9386            4.55m\n",
      "        30           0.8543            4.01m\n",
      "        40           0.7962            3.46m\n",
      "        50           0.7546            2.89m\n",
      "        60           0.7195            2.31m\n",
      "        70           0.6921            1.75m\n",
      "        80           0.6705            1.17m\n",
      "        90           0.6515           35.22s\n",
      "       100           0.6322            0.00s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(verbose=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.fit(embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c99eb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8795796234126405"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.score(embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c503f0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8057742782152231"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.score(val_embeddings, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8274cc",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbda59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { 'n_estimators': [100, 300, 500, 1000],\n",
    "               'max_depth': [300, 500, 1000]\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c1710",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b064077e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   25.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=100; total time=  26.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   24.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=100; total time=  24.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   24.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=100; total time=  24.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   26.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=100; total time=  27.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   28.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=100; total time=  28.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=300; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=300, n_estimators=500; total time= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  2.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=300, n_estimators=1000; total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=300, n_estimators=1000; total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=300, n_estimators=1000; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=300, n_estimators=1000; total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=300, n_estimators=1000; total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   26.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=100; total time=  26.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   26.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=100; total time=  26.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   27.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=100; total time=  27.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   26.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=100; total time=  26.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   25.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=100; total time=  25.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=500; total time= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  3.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=500, n_estimators=1000; total time= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  3.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=500, n_estimators=1000; total time= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=500, n_estimators=1000; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=500, n_estimators=1000; total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=500, n_estimators=1000; total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   26.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=100; total time=  26.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   26.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=100; total time=  27.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   25.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=100; total time=  26.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   26.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=100; total time=  26.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   26.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=100; total time=  26.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=300; total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=300; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=500; total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...................max_depth=1000, n_estimators=500; total time= 2.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..................max_depth=1000, n_estimators=1000; total time= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  3.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..................max_depth=1000, n_estimators=1000; total time= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..................max_depth=1000, n_estimators=1000; total time= 4.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..................max_depth=1000, n_estimators=1000; total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..................max_depth=1000, n_estimators=1000; total time= 4.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=RandomForestClassifier(verbose=1),\n",
       "             param_grid={&#x27;max_depth&#x27;: [300, 500, 1000],\n",
       "                         &#x27;n_estimators&#x27;: [100, 300, 500, 1000]},\n",
       "             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=RandomForestClassifier(verbose=1),\n",
       "             param_grid={&#x27;max_depth&#x27;: [300, 500, 1000],\n",
       "                         &#x27;n_estimators&#x27;: [100, 300, 500, 1000]},\n",
       "             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(verbose=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(verbose=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(verbose=1),\n",
       "             param_grid={'max_depth': [300, 500, 1000],\n",
       "                         'n_estimators': [100, 300, 500, 1000]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(RandomForestClassifier(verbose=1), param_grid, verbose=2)\n",
    "\n",
    "grid.fit(embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ab87ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 500, 'n_estimators': 1000}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15c696c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8110236220472441"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(val_embeddings, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364fc280",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9d78d",
   "metadata": {},
   "source": [
    "Due to the time of computing needed to run GridSearchCV on GradientBoostingClassifier with our param_grid, we are going to use HalvingGridSearchCV and a reduced param_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b79764e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_param_grid = { 'n_estimators': [100, 300],\n",
    "                      'max_depth': [300, 500]\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e77d4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 2283\n",
      "max_resources_: 6851\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 4\n",
      "n_resources: 2283\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1698            1.61m\n",
      "         2           1.0160            1.58m\n",
      "         3           0.8884            1.56m\n",
      "         4           0.7811            1.53m\n",
      "         5           0.6896            1.51m\n",
      "         6           0.6109            1.50m\n",
      "         7           0.5427            1.51m\n",
      "         8           0.4832            1.53m\n",
      "         9           0.4312            1.51m\n",
      "        10           0.3854            1.48m\n",
      "        20           0.1332            1.32m\n",
      "        30           0.0488            1.20m\n",
      "        40           0.0188            1.05m\n",
      "        50           0.0078           52.61s\n",
      "        60           0.0038           42.08s\n",
      "        70           0.0024           31.57s\n",
      "        80           0.0018           21.09s\n",
      "        90           0.0016           10.55s\n",
      "       100           0.0016            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=100; total time= 1.8min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1830            1.52m\n",
      "         2           1.0284            1.63m\n",
      "         3           0.8998            1.59m\n",
      "         4           0.7913            1.61m\n",
      "         5           0.6988            1.61m\n",
      "         6           0.6191            1.61m\n",
      "         7           0.5501            1.57m\n",
      "         8           0.4899            1.54m\n",
      "         9           0.4371            1.54m\n",
      "        10           0.3907            1.52m\n",
      "        20           0.1350            1.29m\n",
      "        30           0.0495            1.12m\n",
      "        40           0.0190           56.74s\n",
      "        50           0.0079           46.75s\n",
      "        60           0.0039           37.47s\n",
      "        70           0.0024           27.98s\n",
      "        80           0.0018           18.59s\n",
      "        90           0.0016            9.26s\n",
      "       100           0.0016            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=100; total time= 1.5min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1734            1.41m\n",
      "         2           1.0194            1.39m\n",
      "         3           0.8917            1.33m\n",
      "         4           0.7841            1.32m\n",
      "         5           0.6924            1.29m\n",
      "         6           0.6135            1.27m\n",
      "         7           0.5451            1.27m\n",
      "         8           0.4855            1.25m\n",
      "         9           0.4333            1.24m\n",
      "        10           0.3874            1.22m\n",
      "        20           0.1345            1.06m\n",
      "        30           0.0499           55.49s\n",
      "        40           0.0198           47.51s\n",
      "        50           0.0088           39.68s\n",
      "        60           0.0048           31.74s\n",
      "        70           0.0033           23.91s\n",
      "        80           0.0028           16.21s\n",
      "        90           0.0026            8.26s\n",
      "       100           0.0025            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=100; total time= 1.4min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1833            1.81m\n",
      "         2           1.0285            1.81m\n",
      "         3           0.8997            2.19m\n",
      "         4           0.7911            2.05m\n",
      "         5           0.6984            2.00m\n",
      "         6           0.6186            1.97m\n",
      "         7           0.5495            1.91m\n",
      "         8           0.4892            1.85m\n",
      "         9           0.4363            1.81m\n",
      "        10           0.3899            1.79m\n",
      "        20           0.1337            1.52m\n",
      "        30           0.0480            1.31m\n",
      "        40           0.0175            1.11m\n",
      "        50           0.0064           55.62s\n",
      "        60           0.0024           44.36s\n",
      "        70           0.0009           33.12s\n",
      "        80           0.0003           21.98s\n",
      "        90           0.0001           10.98s\n",
      "       100           0.0000            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=100; total time= 1.8min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1815            1.76m\n",
      "         2           1.0270            1.73m\n",
      "         3           0.8986            1.75m\n",
      "         4           0.7903            1.73m\n",
      "         5           0.6979            1.72m\n",
      "         6           0.6184            1.70m\n",
      "         7           0.5495            1.67m\n",
      "         8           0.4894            1.65m\n",
      "         9           0.4368            1.61m\n",
      "        10           0.3905            1.59m\n",
      "        20           0.1353            1.40m\n",
      "        30           0.0499            1.22m\n",
      "        40           0.0195            1.05m\n",
      "        50           0.0085           52.12s\n",
      "        60           0.0044           41.66s\n",
      "        70           0.0030           31.38s\n",
      "        80           0.0024           20.86s\n",
      "        90           0.0022           10.41s\n",
      "       100           0.0021            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=100; total time= 1.7min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1698            5.67m\n",
      "         2           1.0160            5.54m\n",
      "         3           0.8884            5.45m\n",
      "         4           0.7811            5.33m\n",
      "         5           0.6896            5.33m\n",
      "         6           0.6109            5.40m\n",
      "         7           0.5427            5.47m\n",
      "         8           0.4832            5.62m\n",
      "         9           0.4312            5.56m\n",
      "        10           0.3854            5.51m\n",
      "        20           0.1332            5.16m\n",
      "        30           0.0488            4.96m\n",
      "        40           0.0188            4.73m\n",
      "        50           0.0078            4.56m\n",
      "        60           0.0038            4.38m\n",
      "        70           0.0024            4.19m\n",
      "        80           0.0018            4.02m\n",
      "        90           0.0016            3.83m\n",
      "       100           0.0016            3.65m\n",
      "       200           0.0015            1.63m\n",
      "       300           0.0015            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=300; total time= 4.0min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1830            4.33m\n",
      "         2           1.0284            4.74m\n",
      "         3           0.8998            4.70m\n",
      "         4           0.7913            4.66m\n",
      "         5           0.6988            4.83m\n",
      "         6           0.6191            4.77m\n",
      "         7           0.5501            4.71m\n",
      "         8           0.4899            5.12m\n",
      "         9           0.4371            5.10m\n",
      "        10           0.3907            5.09m\n",
      "        20           0.1350            4.72m\n",
      "        30           0.0495            4.43m\n",
      "        40           0.0190            4.24m\n",
      "        50           0.0079            4.06m\n",
      "        60           0.0039            3.89m\n",
      "        70           0.0024            3.69m\n",
      "        80           0.0018            3.50m\n",
      "        90           0.0016            3.33m\n",
      "       100           0.0016            3.15m\n",
      "       200           0.0015            1.40m\n",
      "       300           0.0015            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=300; total time= 3.5min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1734            4.23m\n",
      "         2           1.0194            4.19m\n",
      "         3           0.8917            4.10m\n",
      "         4           0.7841            4.14m\n",
      "         5           0.6924            4.10m\n",
      "         6           0.6135            4.04m\n",
      "         7           0.5451            4.04m\n",
      "         8           0.4855            4.00m\n",
      "         9           0.4333            3.96m\n",
      "        10           0.3874            3.94m\n",
      "        20           0.1345            3.73m\n",
      "        30           0.0499            3.57m\n",
      "        40           0.0198            3.43m\n",
      "        50           0.0088            3.29m\n",
      "        60           0.0048            3.15m\n",
      "        70           0.0033            3.01m\n",
      "        80           0.0028            2.93m\n",
      "        90           0.0026            2.85m\n",
      "       100           0.0025            2.76m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       200           0.0025            1.31m\n",
      "       300           0.0025            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=300; total time= 3.3min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1833            6.29m\n",
      "         2           1.0285            6.16m\n",
      "         3           0.8997            7.28m\n",
      "         4           0.7911            6.74m\n",
      "         5           0.6984            6.55m\n",
      "         6           0.6186            6.35m\n",
      "         7           0.5495            6.23m\n",
      "         8           0.4892            6.10m\n",
      "         9           0.4363            5.95m\n",
      "        10           0.3899            5.85m\n",
      "        20           0.1337            5.35m\n",
      "        30           0.0480            5.09m\n",
      "        40           0.0175            4.84m\n",
      "        50           0.0064            4.60m\n",
      "        60           0.0024            4.43m\n",
      "        70           0.0009            4.24m\n",
      "        80           0.0003            4.05m\n",
      "        90           0.0001            3.88m\n",
      "       100           0.0000            3.69m\n",
      "       200           0.0000            1.58m\n",
      "       300           0.0000            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=300; total time= 3.2min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1815            5.38m\n",
      "         2           1.0270            5.36m\n",
      "         3           0.8986            5.35m\n",
      "         4           0.7903            5.32m\n",
      "         5           0.6979            5.31m\n",
      "         6           0.6184            5.27m\n",
      "         7           0.5495            5.24m\n",
      "         8           0.4894            5.22m\n",
      "         9           0.4368            5.19m\n",
      "        10           0.3905            5.19m\n",
      "        20           0.1353            4.90m\n",
      "        30           0.0499            4.73m\n",
      "        40           0.0195            4.55m\n",
      "        50           0.0085            4.35m\n",
      "        60           0.0044            4.17m\n",
      "        70           0.0030            3.99m\n",
      "        80           0.0024            3.81m\n",
      "        90           0.0022            3.65m\n",
      "       100           0.0021            3.46m\n",
      "       200           0.0021            1.55m\n",
      "       300           0.0021            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=300; total time= 4.0min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1698            1.76m\n",
      "         2           1.0160            1.73m\n",
      "         3           0.8884            1.82m\n",
      "         4           0.7811            1.79m\n",
      "         5           0.6896            1.78m\n",
      "         6           0.6109            1.76m\n",
      "         7           0.5427            1.75m\n",
      "         8           0.4832            1.78m\n",
      "         9           0.4312            1.74m\n",
      "        10           0.3854            1.73m\n",
      "        20           0.1332            1.47m\n",
      "        30           0.0488            1.28m\n",
      "        40           0.0188            1.09m\n",
      "        50           0.0078           54.25s\n",
      "        60           0.0038           43.48s\n",
      "        70           0.0024           32.54s\n",
      "        80           0.0018           21.55s\n",
      "        90           0.0016           10.80s\n",
      "       100           0.0016            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=100; total time= 1.8min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1830            1.40m\n",
      "         2           1.0284            1.42m\n",
      "         3           0.8998            1.44m\n",
      "         4           0.7913            1.41m\n",
      "         5           0.6988            1.40m\n",
      "         6           0.6191            1.38m\n",
      "         7           0.5501            1.37m\n",
      "         8           0.4899            1.36m\n",
      "         9           0.4371            1.37m\n",
      "        10           0.3907            1.36m\n",
      "        20           0.1350            1.27m\n",
      "        30           0.0495            1.11m\n",
      "        40           0.0190           56.82s\n",
      "        50           0.0079           47.25s\n",
      "        60           0.0039           37.78s\n",
      "        70           0.0024           28.19s\n",
      "        80           0.0018           18.68s\n",
      "        90           0.0016            9.30s\n",
      "       100           0.0016            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=100; total time= 1.5min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1734            1.46m\n",
      "         2           1.0194            1.44m\n",
      "         3           0.8917            1.38m\n",
      "         4           0.7841            1.38m\n",
      "         5           0.6924            1.34m\n",
      "         6           0.6135            1.31m\n",
      "         7           0.5451            1.29m\n",
      "         8           0.4855            1.27m\n",
      "         9           0.4333            1.25m\n",
      "        10           0.3874            1.23m\n",
      "        20           0.1345            1.08m\n",
      "        30           0.0499           56.38s\n",
      "        40           0.0198           47.94s\n",
      "        50           0.0088           39.71s\n",
      "        60           0.0048           31.78s\n",
      "        70           0.0033           23.76s\n",
      "        80           0.0028           16.04s\n",
      "        90           0.0026            8.21s\n",
      "       100           0.0025            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=100; total time= 1.4min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1833            2.00m\n",
      "         2           1.0285            2.03m\n",
      "         3           0.8997            2.39m\n",
      "         4           0.7911            2.22m\n",
      "         5           0.6984            2.13m\n",
      "         6           0.6186            2.03m\n",
      "         7           0.5495            1.99m\n",
      "         8           0.4892            1.96m\n",
      "         9           0.4363            1.91m\n",
      "        10           0.3899            1.89m\n",
      "        20           0.1337            1.59m\n",
      "        30           0.0480            1.35m\n",
      "        40           0.0175            1.13m\n",
      "        50           0.0064           56.01s\n",
      "        60           0.0024           44.49s\n",
      "        70           0.0009           33.06s\n",
      "        80           0.0003           22.05s\n",
      "        90           0.0001           10.97s\n",
      "       100           0.0000            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=100; total time= 1.8min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1815            1.77m\n",
      "         2           1.0270            1.75m\n",
      "         3           0.8986            1.75m\n",
      "         4           0.7903            1.72m\n",
      "         5           0.6979            1.71m\n",
      "         6           0.6184            1.69m\n",
      "         7           0.5495            1.66m\n",
      "         8           0.4894            1.64m\n",
      "         9           0.4368            1.62m\n",
      "        10           0.3905            1.60m\n",
      "        20           0.1353            1.41m\n",
      "        30           0.0499            1.22m\n",
      "        40           0.0195            1.04m\n",
      "        50           0.0085           51.92s\n",
      "        60           0.0044           41.60s\n",
      "        70           0.0030           31.21s\n",
      "        80           0.0024           20.81s\n",
      "        90           0.0022           10.37s\n",
      "       100           0.0021            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=100; total time= 1.7min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1698            5.60m\n",
      "         2           1.0160            5.64m\n",
      "         3           0.8884            5.71m\n",
      "         4           0.7811            5.55m\n",
      "         5           0.6896            5.56m\n",
      "         6           0.6109            5.54m\n",
      "         7           0.5427            5.54m\n",
      "         8           0.4832            5.66m\n",
      "         9           0.4312            5.58m\n",
      "        10           0.3854            5.56m\n",
      "        20           0.1332            5.20m\n",
      "        30           0.0488            4.97m\n",
      "        40           0.0188            4.76m\n",
      "        50           0.0078            4.56m\n",
      "        60           0.0038            4.37m\n",
      "        70           0.0024            4.19m\n",
      "        80           0.0018            3.99m\n",
      "        90           0.0016            3.80m\n",
      "       100           0.0016            3.62m\n",
      "       200           0.0015            1.61m\n",
      "       300           0.0015            0.00s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....................max_depth=500, n_estimators=300; total time= 3.9min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1830            4.59m\n",
      "         2           1.0284            5.11m\n",
      "         3           0.8998            4.97m\n",
      "         4           0.7913            4.80m\n",
      "         5           0.6988            4.72m\n",
      "         6           0.6191            4.78m\n",
      "         7           0.5501            4.83m\n",
      "         8           0.4899            4.83m\n",
      "         9           0.4371            4.78m\n",
      "        10           0.3907            4.73m\n",
      "        20           0.1350            4.46m\n",
      "        30           0.0495            4.30m\n",
      "        40           0.0190            4.08m\n",
      "        50           0.0079            3.89m\n",
      "        60           0.0039            3.73m\n",
      "        70           0.0024            3.57m\n",
      "        80           0.0018            3.41m\n",
      "        90           0.0016            3.27m\n",
      "       100           0.0016            3.12m\n",
      "       200           0.0015            1.39m\n",
      "       300           0.0015            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=300; total time= 3.4min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1734            4.19m\n",
      "         2           1.0194            4.09m\n",
      "         3           0.8917            4.02m\n",
      "         4           0.7841            4.09m\n",
      "         5           0.6924            4.11m\n",
      "         6           0.6135            4.09m\n",
      "         7           0.5451            4.09m\n",
      "         8           0.4855            4.08m\n",
      "         9           0.4333            4.03m\n",
      "        10           0.3874            4.01m\n",
      "        20           0.1345            3.73m\n",
      "        30           0.0499            3.58m\n",
      "        40           0.0198            3.44m\n",
      "        50           0.0088            3.31m\n",
      "        60           0.0048            3.16m\n",
      "        70           0.0033            3.03m\n",
      "        80           0.0028            2.94m\n",
      "        90           0.0026            2.87m\n",
      "       100           0.0025            2.77m\n",
      "       200           0.0025            1.32m\n",
      "       300           0.0025            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=300; total time= 3.3min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1833            5.51m\n",
      "         2           1.0285            5.49m\n",
      "         3           0.8997            6.60m\n",
      "         4           0.7911            6.27m\n",
      "         5           0.6984            6.17m\n",
      "         6           0.6186            6.10m\n",
      "         7           0.5495            5.98m\n",
      "         8           0.4892            5.87m\n",
      "         9           0.4363            5.82m\n",
      "        10           0.3899            5.75m\n",
      "        20           0.1337            5.31m\n",
      "        30           0.0480            5.06m\n",
      "        40           0.0175            4.83m\n",
      "        50           0.0064            4.61m\n",
      "        60           0.0024            4.46m\n",
      "        70           0.0009            4.26m\n",
      "        80           0.0003            4.08m\n",
      "        90           0.0001            3.88m\n",
      "       100           0.0000            3.69m\n",
      "       200           0.0000            1.58m\n",
      "       300           0.0000            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=300; total time= 3.2min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1815            5.29m\n",
      "         2           1.0270            5.31m\n",
      "         3           0.8986            5.28m\n",
      "         4           0.7903            5.24m\n",
      "         5           0.6979            5.23m\n",
      "         6           0.6184            5.19m\n",
      "         7           0.5495            5.22m\n",
      "         8           0.4894            5.22m\n",
      "         9           0.4368            5.22m\n",
      "        10           0.3905            5.20m\n",
      "        20           0.1353            4.92m\n",
      "        30           0.0499            4.73m\n",
      "        40           0.0195            4.55m\n",
      "        50           0.0085            4.35m\n",
      "        60           0.0044            4.17m\n",
      "        70           0.0030            3.98m\n",
      "        80           0.0024            3.82m\n",
      "        90           0.0022            3.65m\n",
      "       100           0.0021            3.47m\n",
      "       200           0.0021            1.55m\n",
      "       300           0.0021            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=300; total time= 4.0min\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 2\n",
      "n_resources: 6849\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1785           31.80m\n",
      "         2           1.0249           31.93m\n",
      "         3           0.8973           31.47m\n",
      "         4           0.7897           31.77m\n",
      "         5           0.6980           31.41m\n",
      "         6           0.6190           31.09m\n",
      "         7           0.5506           30.92m\n",
      "         8           0.4910           30.78m\n",
      "         9           0.4388           33.15m\n",
      "        10           0.3928           32.66m\n",
      "        20           0.1397           30.31m\n",
      "        30           0.0550           28.68m\n",
      "        40           0.0248           27.27m\n",
      "        50           0.0139           26.09m\n",
      "        60           0.0099           24.97m\n",
      "        70           0.0084           23.88m\n",
      "        80           0.0079           22.81m\n",
      "        90           0.0077           21.74m\n",
      "       100           0.0076           20.71m\n",
      "       200           0.0075            9.54m\n",
      "       300           0.0075            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=300; total time=25.1min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1784           27.69m\n",
      "         2           1.0246           27.62m\n",
      "         3           0.8969           28.59m\n",
      "         4           0.7893           28.56m\n",
      "         5           0.6975           29.00m\n",
      "         6           0.6185           28.55m\n",
      "         7           0.5501           28.16m\n",
      "         8           0.4904           31.20m\n",
      "         9           0.4381           30.64m\n",
      "        10           0.3921           30.01m\n",
      "        20           0.1388           27.33m\n",
      "        30           0.0540           25.83m\n",
      "        40           0.0239           24.84m\n",
      "        50           0.0129           23.85m\n",
      "        60           0.0089           22.74m\n",
      "        70           0.0074           21.67m\n",
      "        80           0.0068           20.63m\n",
      "        90           0.0066           19.64m\n",
      "       100           0.0066           18.61m\n",
      "       200           0.0065            8.49m\n",
      "       300           0.0065            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=300; total time=23.0min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1788           24.89m\n",
      "         2           1.0253           24.88m\n",
      "         3           0.8979           24.28m\n",
      "         4           0.7904           25.51m\n",
      "         5           0.6988           25.12m\n",
      "         6           0.6199           24.75m\n",
      "         7           0.5516           24.42m\n",
      "         8           0.4920           24.24m\n",
      "         9           0.4399           23.97m\n",
      "        10           0.3940           23.81m\n",
      "        20           0.1411           22.58m\n",
      "        30           0.0566           21.58m\n",
      "        40           0.0265           20.75m\n",
      "        50           0.0155           20.00m\n",
      "        60           0.0115           19.26m\n",
      "        70           0.0100           18.48m\n",
      "        80           0.0095           17.71m\n",
      "        90           0.0093           16.91m\n",
      "       100           0.0092           16.11m\n",
      "       200           0.0092            7.74m\n",
      "       300           0.0092            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=300; total time=22.5min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1787           24.13m\n",
      "         2           1.0251           23.67m\n",
      "         3           0.8976           22.62m\n",
      "         4           0.7901           26.43m\n",
      "         5           0.6984           25.24m\n",
      "         6           0.6195           24.78m\n",
      "         7           0.5511           24.40m\n",
      "         8           0.4915           24.09m\n",
      "         9           0.4393           23.50m\n",
      "        10           0.3934           23.00m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        20           0.1403           21.52m\n",
      "        30           0.0557           21.99m\n",
      "        40           0.0256           23.04m\n",
      "        50           0.0146           23.21m\n",
      "        60           0.0106           22.93m\n",
      "        70           0.0092           22.39m\n",
      "        80           0.0086           21.88m\n",
      "        90           0.0084           20.91m\n",
      "       100           0.0083           19.55m\n",
      "       200           0.0083            8.43m\n",
      "       300           0.0083            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=300; total time=23.8min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1786           27.06m\n",
      "         2           1.0250           27.18m\n",
      "         3           0.8975           27.06m\n",
      "         4           0.7900           27.26m\n",
      "         5           0.6983           27.13m\n",
      "         6           0.6194           27.11m\n",
      "         7           0.5511           26.97m\n",
      "         8           0.4915           26.73m\n",
      "         9           0.4393           26.77m\n",
      "        10           0.3934           26.53m\n",
      "        20           0.1403           25.64m\n",
      "        30           0.0557           24.44m\n",
      "        40           0.0255           23.34m\n",
      "        50           0.0146           22.34m\n",
      "        60           0.0106           21.35m\n",
      "        70           0.0091           20.41m\n",
      "        80           0.0085           19.57m\n",
      "        90           0.0083           18.71m\n",
      "       100           0.0083           17.84m\n",
      "       200           0.0082            8.45m\n",
      "       300           0.0082            0.00s\n",
      "[CV] END ....................max_depth=300, n_estimators=300; total time=23.7min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1785           31.72m\n",
      "         2           1.0249           31.49m\n",
      "         3           0.8973           31.36m\n",
      "         4           0.7897           31.72m\n",
      "         5           0.6980           31.46m\n",
      "         6           0.6190           31.21m\n",
      "         7           0.5506           31.10m\n",
      "         8           0.4910           30.90m\n",
      "         9           0.4388           33.12m\n",
      "        10           0.3928           32.70m\n",
      "        20           0.1397           30.23m\n",
      "        30           0.0550           28.68m\n",
      "        40           0.0248           27.28m\n",
      "        50           0.0139           25.98m\n",
      "        60           0.0099           24.81m\n",
      "        70           0.0084           23.75m\n",
      "        80           0.0079           22.77m\n",
      "        90           0.0077           21.74m\n",
      "       100           0.0076           20.70m\n",
      "       200           0.0075            9.51m\n",
      "       300           0.0075            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=300; total time=25.2min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1784           26.20m\n",
      "         2           1.0246           26.48m\n",
      "         3           0.8969           27.05m\n",
      "         4           0.7893           28.03m\n",
      "         5           0.6975           28.60m\n",
      "         6           0.6185           28.82m\n",
      "         7           0.5501           28.33m\n",
      "         8           0.4904           31.86m\n",
      "         9           0.4381           31.23m\n",
      "        10           0.3921           30.67m\n",
      "        20           0.1388           28.14m\n",
      "        30           0.0540           26.23m\n",
      "        40           0.0239           24.68m\n",
      "        50           0.0129           23.44m\n",
      "        60           0.0089           22.30m\n",
      "        70           0.0074           21.29m\n",
      "        80           0.0068           20.42m\n",
      "        90           0.0066           19.54m\n",
      "       100           0.0066           18.61m\n",
      "       200           0.0065            8.48m\n",
      "       300           0.0065            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=300; total time=23.2min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1788           25.17m\n",
      "         2           1.0253           25.42m\n",
      "         3           0.8979           24.80m\n",
      "         4           0.7904           26.86m\n",
      "         5           0.6988           26.26m\n",
      "         6           0.6199           25.94m\n",
      "         7           0.5516           25.66m\n",
      "         8           0.4920           25.40m\n",
      "         9           0.4399           25.16m\n",
      "        10           0.3940           24.90m\n",
      "        20           0.1411           23.47m\n",
      "        30           0.0566           22.38m\n",
      "        40           0.0265           21.46m\n",
      "        50           0.0155           20.52m\n",
      "        60           0.0115           19.55m\n",
      "        70           0.0100           18.66m\n",
      "        80           0.0095           17.76m\n",
      "        90           0.0093           16.90m\n",
      "       100           0.0092           16.09m\n",
      "       200           0.0092            7.69m\n",
      "       300           0.0092            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=300; total time=22.1min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1787           19.93m\n",
      "         2           1.0251           21.29m\n",
      "         3           0.8976           21.62m\n",
      "         4           0.7901           23.89m\n",
      "         5           0.6984           23.06m\n",
      "         6           0.6195           22.75m\n",
      "         7           0.5511           22.58m\n",
      "         8           0.4915           22.15m\n",
      "         9           0.4393           21.75m\n",
      "        10           0.3934           21.41m\n",
      "        20           0.1403           19.82m\n",
      "        30           0.0557           20.93m\n",
      "        40           0.0256           22.37m\n",
      "        50           0.0146           22.93m\n",
      "        60           0.0106           22.79m\n",
      "        70           0.0092           22.11m\n",
      "        80           0.0086           21.25m\n",
      "        90           0.0084           20.36m\n",
      "       100           0.0083           19.03m\n",
      "       200           0.0083            8.32m\n",
      "       300           0.0083            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=300; total time=23.4min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1786           26.96m\n",
      "         2           1.0250           27.17m\n",
      "         3           0.8975           26.77m\n",
      "         4           0.7900           26.74m\n",
      "         5           0.6983           26.47m\n",
      "         6           0.6194           26.47m\n",
      "         7           0.5511           26.18m\n",
      "         8           0.4915           25.99m\n",
      "         9           0.4393           25.85m\n",
      "        10           0.3934           25.70m\n",
      "        20           0.1403           24.70m\n",
      "        30           0.0557           23.76m\n",
      "        40           0.0255           23.01m\n",
      "        50           0.0146           22.22m\n",
      "        60           0.0106           21.42m\n",
      "        70           0.0091           20.56m\n",
      "        80           0.0085           19.68m\n",
      "        90           0.0083           18.73m\n",
      "       100           0.0083           17.82m\n",
      "       200           0.0082            8.53m\n",
      "       300           0.0082            0.00s\n",
      "[CV] END ....................max_depth=500, n_estimators=300; total time=24.2min\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1788           34.21m\n",
      "         2           1.0253           34.28m\n",
      "         3           0.8979           33.65m\n",
      "         4           0.7904           35.59m\n",
      "         5           0.6988           35.31m\n",
      "         6           0.6200           35.07m\n",
      "         7           0.5517           34.74m\n",
      "         8           0.4921           34.60m\n",
      "         9           0.4399           34.28m\n",
      "        10           0.3941           36.90m\n",
      "        20           0.1412           33.88m\n",
      "        30           0.0567           32.11m\n",
      "        40           0.0266           30.62m\n",
      "        50           0.0156           29.19m\n",
      "        60           0.0116           27.75m\n",
      "        70           0.0101           26.32m\n",
      "        80           0.0096           24.96m\n",
      "        90           0.0094           23.69m\n",
      "       100           0.0093           22.46m\n",
      "       200           0.0093           10.36m\n",
      "       300           0.0093            0.00s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(estimator=GradientBoostingClassifier(verbose=1),\n",
       "                    param_grid={&#x27;max_depth&#x27;: [300, 500],\n",
       "                                &#x27;n_estimators&#x27;: [100, 300]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(estimator=GradientBoostingClassifier(verbose=1),\n",
       "                    param_grid={&#x27;max_depth&#x27;: [300, 500],\n",
       "                                &#x27;n_estimators&#x27;: [100, 300]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(verbose=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(verbose=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(estimator=GradientBoostingClassifier(verbose=1),\n",
       "                    param_grid={'max_depth': [300, 500],\n",
       "                                'n_estimators': [100, 300]},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = HalvingGridSearchCV(GradientBoostingClassifier(verbose=1), reduced_param_grid, verbose=2)\n",
    "\n",
    "grid.fit(embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e0eca8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 300, 'n_estimators': 300}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee17307b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7230971128608924"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(val_embeddings, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193eac57",
   "metadata": {},
   "source": [
    "# Fine-Tuning BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5be8c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e6c4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data.text.values\n",
    "labels = data.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b3473144",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ba81dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Token IDs: [101, 2256, 15616, 2024, 1996, 3114, 1997, 2023, 1001, 8372, 2089, 16455, 9641, 2149, 2035, 102]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3af0cac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  84\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "71165ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 84\n",
    "\n",
    "#Padding the input to the max length that is 84\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "64ef0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "184c8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=42, test_size=0.2)\n",
    "# Performing same steps on the attention masks\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
    "                                             random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "74652b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the input data to the tensor , which can be feeded to the model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f01cdc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the DataLoader which will help us to load data into the GPU/CPU\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "382c982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2,   \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False, )\n",
    "\n",
    "# Telling the model to run on GPU \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ec07220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leodray/miniforge3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# AdamW is an optimizer which is a Adam Optimzier with weight-decay-fix\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-6, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c8abd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x2e2304ac0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 5\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6bf4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c67779ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the helper function to have a watch on elapsed time\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "16244b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    191.    Elapsed: 0:03:29.\n",
      "  Batch    80  of    191.    Elapsed: 0:06:53.\n",
      "  Batch   120  of    191.    Elapsed: 0:10:29.\n",
      "  Batch   160  of    191.    Elapsed: 0:13:50.\n",
      "\n",
      "  Average training loss: 0.6028\n",
      "  Training epoch took: 0:16:24\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.7905\n",
      "  Validation took: 0:00:50\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    191.    Elapsed: 0:03:28.\n",
      "  Batch    80  of    191.    Elapsed: 0:06:49.\n",
      "  Batch   120  of    191.    Elapsed: 0:10:17.\n",
      "  Batch   160  of    191.    Elapsed: 0:13:55.\n",
      "\n",
      "  Average training loss: 0.4741\n",
      "  Training epoch took: 0:16:24\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8120\n",
      "  Validation took: 0:00:30\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    191.    Elapsed: 0:01:25.\n",
      "  Batch    80  of    191.    Elapsed: 0:02:50.\n",
      "  Batch   120  of    191.    Elapsed: 0:04:14.\n",
      "  Batch   160  of    191.    Elapsed: 0:05:39.\n",
      "\n",
      "  Average training loss: 0.4290\n",
      "  Training epoch took: 0:06:43\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8222\n",
      "  Validation took: 0:00:30\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    191.    Elapsed: 0:01:25.\n",
      "  Batch    80  of    191.    Elapsed: 0:02:50.\n",
      "  Batch   120  of    191.    Elapsed: 0:04:14.\n",
      "  Batch   160  of    191.    Elapsed: 0:05:47.\n",
      "\n",
      "  Average training loss: 0.4033\n",
      "  Training epoch took: 0:07:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8242\n",
      "  Validation took: 0:01:12\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    191.    Elapsed: 0:09:29.\n",
      "  Batch    80  of    191.    Elapsed: 0:20:20.\n",
      "  Batch   120  of    191.    Elapsed: 0:34:08.\n",
      "  Batch   160  of    191.    Elapsed: 0:48:20.\n",
      "\n",
      "  Average training loss: 0.4003\n",
      "  Training epoch took: 0:58:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.8261\n",
      "  Validation took: 0:03:08\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "#Let's start the training process\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 0\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd0fb9",
   "metadata": {},
   "source": [
    "# Making predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1c6df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b9b64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82f9fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in test_sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c295c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e10bffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf97f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1623ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "efe867da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size.  \n",
    "batch_size = 32  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bbebd889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3cf197bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b579af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "de9c47bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868378b",
   "metadata": {},
   "source": [
    "# Saving predictions in the sample submission CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "02f6d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "19416356",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.Series(flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c600e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['target'] = submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "68f72364",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('data/sample_submission_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
